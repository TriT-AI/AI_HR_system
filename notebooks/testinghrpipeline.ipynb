{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1f5ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (0.3.28)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (0.3.27)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langsmith in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (0.4.13)\n",
      "Collecting langsmith\n",
      "  Downloading langsmith-0.4.16-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: azure-core in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (1.34.0)\n",
      "Collecting azure-core\n",
      "  Downloading azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: azure-ai-documentintelligence in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (1.0.2)\n",
      "Collecting duckdb\n",
      "  Downloading duckdb-1.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain-openai)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
      "  Downloading openai-1.101.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from langsmith) (0.23.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from azure-core) (1.17.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/tritr/miniconda3/envs/dev/lib/python3.11/site-packages (from azure-ai-documentintelligence) (0.7.2)\n",
      "Downloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Downloading openai-1.101.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.8/810.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.3-py3-none-any.whl (52 kB)\n",
      "Downloading langsmith-0.4.16-py3-none-any.whl (375 kB)\n",
      "Downloading azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
      "Downloading duckdb-1.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m22.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:06\u001b[0m00:14\u001b[0mm\n",
      "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, duckdb, azure-core, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph\n",
      "\u001b[2K  Attempting uninstall: azure-core━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [duckdb]\n",
      "\u001b[2K    Found existing installation: azure-core 1.34.0━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [duckdb]\n",
      "\u001b[2K    Uninstalling azure-core-1.34.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [duckdb]\n",
      "\u001b[2K      Successfully uninstalled azure-core-1.34.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [duckdb]\n",
      "\u001b[2K  Attempting uninstall: openai0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [azure-core]\n",
      "\u001b[2K    Found existing installation: openai 1.99.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [azure-core]\n",
      "\u001b[2K    Uninstalling openai-1.99.1:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.99.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: langsmith0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K    Found existing installation: langsmith 0.4.13━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K    Uninstalling langsmith-0.4.13:0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.4.13━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: langchain-core\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [langsmith]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.72━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [langsmith]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.72:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [langsmith]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.72━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [langsmith]\n",
      "\u001b[2K  Attempting uninstall: langchain-openai╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-openai 0.3.28━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-openai-0.3.28:\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-openai-0.3.28━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [langchain-core]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [langgraph]12\u001b[0m [langgraph]prebuilt]\n",
      "\u001b[1A\u001b[2KSuccessfully installed azure-core-1.35.0 duckdb-1.3.2 langchain-core-0.3.74 langchain-openai-0.3.31 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.3 langsmith-0.4.16 openai-1.101.0 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-openai langchain langgraph langsmith azure-core azure-ai-documentintelligence duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742e6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b817b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee09176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.ai.documentintelligence._patch.DocumentIntelligenceClient at 0x7349119dd510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azure Document Intelligence configuration\n",
    "DOCUMENT_INTELLIGENCE_KEY = \"FM58vRr29iHOMbZsbsG1m63gkwUkAtQt8JpWGFyB53cEAqlX16p4JQQJ99BHACYeBjFXJ3w3AAALACOG4Tyz\"\n",
    "DOCUMENT_INTELLIGENCE_ENDPOINT = \"https://tritdocintel.cognitiveservices.azure.com/\"\n",
    "\n",
    "# Initialize Azure Document Intelligence client\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "    credential=AzureKeyCredential(DOCUMENT_INTELLIGENCE_KEY)\n",
    ")\n",
    "document_intelligence_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo LLM với Langchain (sử dụng Azure OpenAI)\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=\"B1NWhvI61o8yCmsHg4Fa3StgdbPusXhLoUnfRkbYEsNuBzgPbyWmJQQJ99BGACHYHv6XJ3w3AAABACOGHGvM\",\n",
    "    azure_endpoint=\"https://yvstritopenai.openai.azure.com/\",\n",
    "    deployment_name=\"gpt-4.1-mini\",  # Thay bằng tên deployment thực tế của bạn\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0858cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models for Structured Output (same as before)\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for the candidate.\"\"\"\n",
    "    email: str = Field(description=\"Candidate's email address\", default=\"\")\n",
    "    phone: str = Field(description=\"Candidate's phone number\", default=\"\")\n",
    "\n",
    "class EmploymentRecord(BaseModel):\n",
    "    \"\"\"Employment history record.\"\"\"\n",
    "    position: str = Field(description=\"Job title/position\", default=\"\")\n",
    "    employer: str = Field(description=\"Company/employer name\", default=\"\")\n",
    "    start_date: str = Field(description=\"Start date (YYYY-MM-DD format)\", default=\"\")\n",
    "    end_date: str = Field(description=\"End date (YYYY-MM-DD or 'Present')\", default=\"\")\n",
    "    summary: str = Field(description=\"Job description and responsibilities\", default=\"\")\n",
    "\n",
    "class EducationRecord(BaseModel):\n",
    "    \"\"\"Education history record.\"\"\"\n",
    "    institution: str = Field(description=\"Educational institution name\", default=\"\")\n",
    "    degree: str = Field(description=\"Degree or field of study\", default=\"\")\n",
    "    graduation_year: str = Field(description=\"Graduation year\", default=\"\")\n",
    "\n",
    "class Project(BaseModel):\n",
    "    \"\"\"Project information.\"\"\"\n",
    "    name: str = Field(description=\"Project name\", default=\"\")\n",
    "    description: str = Field(description=\"Project description\", default=\"\")\n",
    "\n",
    "class Employment(BaseModel):\n",
    "    \"\"\"Employment information container.\"\"\"\n",
    "    history: List[EmploymentRecord] = Field(description=\"List of employment records\", default_factory=list)\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Education information container.\"\"\"\n",
    "    history: List[EducationRecord] = Field(description=\"List of education records\", default_factory=list)\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    \"\"\"Complete resume structure following industry standards.\"\"\"\n",
    "    name: str = Field(description=\"Full name of the candidate\", default=\"\")\n",
    "    contact: ContactInfo = Field(description=\"Contact information\", default_factory=ContactInfo)\n",
    "    employment: Employment = Field(description=\"Employment history\", default_factory=Employment)\n",
    "    education: Education = Field(description=\"Education history\", default_factory=Education)\n",
    "    skills: List[str] = Field(description=\"List of skills and competencies\", default_factory=list)\n",
    "    projects: List[Project] = Field(description=\"List of notable projects\", default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1280f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State Definition\n",
    "class ProcessingState(TypedDict):\n",
    "    \"\"\"State container for the CV processing workflow.\"\"\"\n",
    "    pdf_path: str\n",
    "    extracted_text: Annotated[str, \"Document Intelligence extracted text\"]\n",
    "    structured_data: Annotated[Optional[dict], \"Parsed resume data\"]\n",
    "    error_message: Annotated[Optional[str], \"Error information\"]\n",
    "    processing_status: Annotated[str, \"Current processing status\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7514659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template with Clear Instructions\n",
    "EXTRACTION_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an expert HR data extraction specialist. Extract information from the following resume text and structure it according to the provided schema.\n",
    "\n",
    "Resume Text:\n",
    "{resume_text}\n",
    "\n",
    "Instructions:\n",
    "- Extract all available information accurately\n",
    "- Use empty strings for missing text fields\n",
    "- Use empty arrays for missing list fields\n",
    "- Format dates as YYYY-MM-DD when possible\n",
    "- For current positions, use \"Present\" as end date\n",
    "- Be precise and avoid hallucinations\n",
    "\n",
    "Resume text to process:\n",
    "{resume_text}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(state: ProcessingState) -> ProcessingState:\n",
    "    \"\"\"\n",
    "    Extract text from PDF using Azure Document Intelligence.\n",
    "    \n",
    "    Args:\n",
    "        state: Current processing state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with extracted text or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting Azure Document Intelligence extraction for: {state['pdf_path']}\")\n",
    "        \n",
    "        # Validate file exists\n",
    "        pdf_path = Path(state[\"pdf_path\"])\n",
    "        if not pdf_path.exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        # Read PDF file\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            # Use prebuilt-layout model for comprehensive text extraction with layout information\n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", \n",
    "                body=f\n",
    "            )\n",
    "            \n",
    "            # Get the analysis result\n",
    "            result: AnalyzeResult = poller.result()\n",
    "        \n",
    "        # Extract text from the analysis result\n",
    "        extracted_text = \"\"\n",
    "        \n",
    "        # Method 1: Extract from paragraphs (preserves document structure)\n",
    "        if result.paragraphs:\n",
    "            logger.info(f\"Found {len(result.paragraphs)} paragraphs\")\n",
    "            # Sort paragraphs by their position in the document\n",
    "            sorted_paragraphs = sorted(\n",
    "                result.paragraphs, \n",
    "                key=lambda p: (p.spans[0].offset if p.spans else 0)\n",
    "            )\n",
    "            \n",
    "            for paragraph in sorted_paragraphs:\n",
    "                if paragraph.content:\n",
    "                    extracted_text += paragraph.content + \"\\n\"\n",
    "        \n",
    "        # Method 2: Fallback to pages if no paragraphs found\n",
    "        elif result.pages:\n",
    "            logger.info(f\"Found {len(result.pages)} pages, extracting from lines\")\n",
    "            for page in result.pages:\n",
    "                if page.lines:\n",
    "                    for line in page.lines:\n",
    "                        extracted_text += line.content + \"\\n\"\n",
    "        \n",
    "        # Method 3: Final fallback to raw content if available\n",
    "        elif hasattr(result, 'content') and result.content:\n",
    "            extracted_text = result.content\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            raise ValueError(\"No text could be extracted from the PDF using Azure Document Intelligence\")\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(extracted_text)} characters using Azure Document Intelligence\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"extracted_text\": extracted_text,\n",
    "            \"processing_status\": \"text_extracted\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Azure Document Intelligence extraction failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": error_msg,\n",
    "            \"processing_status\": \"ocr_failed\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56983d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_resume_data(state: ProcessingState) -> ProcessingState:\n",
    "    \"\"\"\n",
    "    Parse extracted text into structured resume data using LLM.\n",
    "    \n",
    "    Args:\n",
    "        state: Current processing state with extracted text\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with structured data or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting LLM-based data structuring\")\n",
    "        \n",
    "        # Create structured output chain\n",
    "        structured_llm = llm.with_structured_output(\n",
    "            Resume)\n",
    "        \n",
    "        # Build processing chain\n",
    "        extraction_chain = EXTRACTION_PROMPT | structured_llm\n",
    "        \n",
    "        # Process the extracted text\n",
    "        structured_resume = extraction_chain.invoke({\n",
    "            \"resume_text\": state[\"extracted_text\"]\n",
    "        })\n",
    "        \n",
    "        # Convert to dictionary for JSON serialization\n",
    "        structured_data = structured_resume.dict()\n",
    "        \n",
    "        logger.info(\"Successfully structured resume data\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"structured_data\": structured_data,\n",
    "            \"processing_status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Data structuring failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": error_msg,\n",
    "            \"processing_status\": \"structuring_failed\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a5b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_processing(state: ProcessingState) -> str:\n",
    "    \"\"\"\n",
    "    Determine the next step in the workflow.\n",
    "    \n",
    "    Args:\n",
    "        state: Current processing state\n",
    "        \n",
    "    Returns:\n",
    "        Next node name or END\n",
    "    \"\"\"\n",
    "    if state.get(\"error_message\"):\n",
    "        return END\n",
    "    \n",
    "    status = state.get(\"processing_status\", \"\")\n",
    "    \n",
    "    if status == \"text_extracted\":\n",
    "        return \"structure_data\"\n",
    "    elif status in [\"completed\", \"ocr_failed\", \"structuring_failed\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"extract_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2ea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LangGraph Workflow\n",
    "def create_cv_processing_workflow() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Create and configure the CV processing workflow.\n",
    "    \n",
    "    Returns:\n",
    "        Compiled StateGraph for CV processing\n",
    "    \"\"\"\n",
    "    workflow = StateGraph(ProcessingState)\n",
    "    \n",
    "    # Add processing nodes\n",
    "    workflow.add_node(\"extract_text\", extract_text_from_pdf)\n",
    "    workflow.add_node(\"structure_data\", structure_resume_data)\n",
    "    \n",
    "    # Define workflow edges\n",
    "    workflow.set_entry_point(\"extract_text\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"extract_text\",\n",
    "        should_continue_processing,\n",
    "        {\n",
    "            \"structure_data\": \"structure_data\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"structure_data\",\n",
    "        should_continue_processing,\n",
    "        {END: END}\n",
    "    )\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e2ad2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing Function\n",
    "class CVProcessor:\n",
    "    \"\"\"Main CV processing class with Azure Document Intelligence workflow management.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the CV processor with workflow.\"\"\"\n",
    "        self.workflow = create_cv_processing_workflow()\n",
    "    \n",
    "    def process_resume(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        output_path: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Process a CV PDF file and extract structured data using Azure Document Intelligence.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            output_path: Optional path to save JSON output\n",
    "            \n",
    "        Returns:\n",
    "            Structured resume data as dictionary\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If processing fails\n",
    "        \"\"\"\n",
    "        # Initialize processing state\n",
    "        initial_state: ProcessingState = {\n",
    "            \"pdf_path\": pdf_path,\n",
    "            \"extracted_text\": \"\",\n",
    "            \"structured_data\": None,\n",
    "            \"error_message\": None,\n",
    "            \"processing_status\": \"initialized\"\n",
    "        }\n",
    "        \n",
    "        # Execute workflow\n",
    "        final_state = self.workflow.invoke(initial_state)\n",
    "        \n",
    "        # Check for errors\n",
    "        if final_state.get(\"error_message\"):\n",
    "            raise ValueError(final_state[\"error_message\"])\n",
    "        \n",
    "        structured_data = final_state.get(\"structured_data\", {})\n",
    "        \n",
    "        # Save to file if requested\n",
    "        if output_path:\n",
    "            output_file = Path(output_path)\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
    "            logger.info(f\"Structured data saved to: {output_file}\")\n",
    "        \n",
    "        return structured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6694bd",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5901b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_processor = CVProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8543844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pdf = \"10265057.pdf\"\n",
    "output_json = \"structured_resume.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a4b586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Azure Document Intelligence extraction for: 10265057.pdf\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com//documentintelligence/documentModels/prebuilt-layout:analyze?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'content-type': 'application/octet-stream'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '0d41939c-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "A body is sent with the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 202\n",
      "Response headers:\n",
      "    'Content-Length': '0'\n",
      "    'Operation-Location': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:35 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/39a44dbc-c431-454e-950f-28928ac4799d?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '0d41939c-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '106'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'retry-after': '1'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:36 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/39a44dbc-c431-454e-950f-28928ac4799d?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '0d41939c-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '106'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'retry-after': '1'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:37 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/39a44dbc-c431-454e-950f-28928ac4799d?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '0d41939c-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '106'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'retry-after': '1'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:39 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/39a44dbc-c431-454e-950f-28928ac4799d?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '0d41939c-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '128157'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:41 GMT'\n",
      "INFO:__main__:Found 20 paragraphs\n",
      "INFO:__main__:Successfully extracted 5154 characters using Azure Document Intelligence\n",
      "INFO:__main__:Starting LLM-based data structuring\n",
      "INFO:httpx:HTTP Request: POST https://yvstritopenai.openai.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2025-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "/tmp/ipykernel_46698/3678663438.py:27: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  structured_data = structured_resume.dict()\n",
      "INFO:__main__:Successfully structured resume data\n",
      "INFO:__main__:Structured data saved to: structured_resume.json\n"
     ]
    }
   ],
   "source": [
    "result = cv_processor.process_resume(input_pdf, output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1c947",
   "metadata": {},
   "source": [
    "# DuckDB Snowflake Schema and Import System for Resume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab556a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fa94985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e000c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeDatabase:\n",
    "    \"\"\"Handles DuckDB operations for resume data with snowflake schema.\"\"\"\n",
    "    \n",
    "    def __init__(self, database_path: str = \"resume_database.db\"):\n",
    "        \"\"\"\n",
    "        Initialize the resume database.\n",
    "        \n",
    "        Args:\n",
    "            database_path: Path to DuckDB file (use ':memory:' for in-memory)\n",
    "        \"\"\"\n",
    "        self.conn = duckdb.connect(database=database_path)\n",
    "        self.create_snowflake_schema()\n",
    "    \n",
    "    def create_snowflake_schema(self):\n",
    "        \"\"\"Create the snowflake schema for resume data.\"\"\"\n",
    "        \n",
    "        # Snowflake schema DDL\n",
    "        schema_ddl = \"\"\"\n",
    "        -- Core candidate information (dimension table)\n",
    "        CREATE TABLE IF NOT EXISTS candidates (\n",
    "            candidate_id INTEGER PRIMARY KEY,\n",
    "            name VARCHAR NOT NULL,\n",
    "            email VARCHAR,\n",
    "            phone VARCHAR,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \n",
    "        -- Work experience fact table\n",
    "        CREATE TABLE IF NOT EXISTS work_experience (\n",
    "            work_exp_id INTEGER PRIMARY KEY,\n",
    "            candidate_id INTEGER NOT NULL,\n",
    "            job_title VARCHAR,\n",
    "            company VARCHAR,\n",
    "            start_date DATE,\n",
    "            end_date DATE,\n",
    "            description TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates(candidate_id)\n",
    "        );\n",
    "        \n",
    "        -- Education fact table\n",
    "        CREATE TABLE IF NOT EXISTS education (\n",
    "            education_id INTEGER PRIMARY KEY,\n",
    "            candidate_id INTEGER NOT NULL,\n",
    "            degree VARCHAR,\n",
    "            institution VARCHAR,\n",
    "            graduation_year VARCHAR,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates(candidate_id)\n",
    "        );\n",
    "        \n",
    "        -- Skills dimension table (normalized)\n",
    "        CREATE TABLE IF NOT EXISTS skills_master (\n",
    "            skill_id INTEGER PRIMARY KEY,\n",
    "            skill_name VARCHAR UNIQUE NOT NULL\n",
    "        );\n",
    "        \n",
    "        -- Candidate-Skills relationship table (many-to-many)\n",
    "        CREATE TABLE IF NOT EXISTS candidate_skills (\n",
    "            candidate_id INTEGER,\n",
    "            skill_id INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (candidate_id, skill_id),\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates(candidate_id),\n",
    "            FOREIGN KEY (skill_id) REFERENCES skills_master(skill_id)\n",
    "        );\n",
    "        \n",
    "        -- Projects fact table\n",
    "        CREATE TABLE IF NOT EXISTS projects (\n",
    "            project_id INTEGER PRIMARY KEY,\n",
    "            candidate_id INTEGER NOT NULL,\n",
    "            project_name VARCHAR,\n",
    "            description TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates(candidate_id)\n",
    "        );\n",
    "        \n",
    "        -- Create sequences for primary keys\n",
    "        CREATE SEQUENCE IF NOT EXISTS candidate_seq START 1;\n",
    "        CREATE SEQUENCE IF NOT EXISTS work_exp_seq START 1;\n",
    "        CREATE SEQUENCE IF NOT EXISTS education_seq START 1;\n",
    "        CREATE SEQUENCE IF NOT EXISTS skill_seq START 1;\n",
    "        CREATE SEQUENCE IF NOT EXISTS project_seq START 1;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute schema creation\n",
    "        statements = schema_ddl.split(';')\n",
    "        for statement in statements:\n",
    "            if statement.strip():\n",
    "                self.conn.execute(statement)\n",
    "        \n",
    "        logger.info(\"Snowflake schema created successfully\")\n",
    "    \n",
    "    def import_resume_data(self, structured_resume: Dict) -> int:\n",
    "        \"\"\"\n",
    "        Import structured resume JSON data into the snowflake schema.\n",
    "        \n",
    "        Args:\n",
    "            structured_resume: Dictionary containing structured resume data\n",
    "            \n",
    "        Returns:\n",
    "            candidate_id of the inserted candidate\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Begin transaction\n",
    "            self.conn.begin()\n",
    "            \n",
    "            # 1. Insert candidate (main dimension)\n",
    "            candidate_id = self.conn.execute(\"SELECT nextval('candidate_seq')\").fetchone()[0]\n",
    "            \n",
    "            candidate_data = {\n",
    "                'candidate_id': candidate_id,\n",
    "                'name': structured_resume.get('name', ''),\n",
    "                'email': structured_resume.get('contact', {}).get('email', ''),\n",
    "                'phone': structured_resume.get('contact', {}).get('phone', '')\n",
    "            }\n",
    "            \n",
    "            self.conn.execute(\n",
    "                \"\"\"INSERT INTO candidates (candidate_id, name, email, phone) \n",
    "                   VALUES ($candidate_id, $name, $email, $phone)\"\"\",\n",
    "                candidate_data\n",
    "            )\n",
    "            \n",
    "            # 2. Insert work experience\n",
    "            employment_history = structured_resume.get('employment', {}).get('history', [])\n",
    "            for work in employment_history:\n",
    "                work_exp_id = self.conn.execute(\"SELECT nextval('work_exp_seq')\").fetchone()[0]\n",
    "                \n",
    "                # Parse dates properly\n",
    "                start_date = self._parse_date(work.get('start', ''))\n",
    "                end_date = self._parse_date(work.get('end', '')) if work.get('end', '').lower() != 'present' else None\n",
    "                \n",
    "                work_data = {\n",
    "                    'work_exp_id': work_exp_id,\n",
    "                    'candidate_id': candidate_id,\n",
    "                    'job_title': work.get('position', ''),\n",
    "                    'company': work.get('employer', ''),\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'description': work.get('summary', '')\n",
    "                }\n",
    "                \n",
    "                self.conn.execute(\n",
    "                    \"\"\"INSERT INTO work_experience \n",
    "                       (work_exp_id, candidate_id, job_title, company, start_date, end_date, description)\n",
    "                       VALUES ($work_exp_id, $candidate_id, $job_title, $company, $start_date, $end_date, $description)\"\"\",\n",
    "                    work_data\n",
    "                )\n",
    "            \n",
    "            # 3. Insert education\n",
    "            education_history = structured_resume.get('education', {}).get('history', [])\n",
    "            for edu in education_history:\n",
    "                education_id = self.conn.execute(\"SELECT nextval('education_seq')\").fetchone()[0]\n",
    "                \n",
    "                edu_data = {\n",
    "                    'education_id': education_id,\n",
    "                    'candidate_id': candidate_id,\n",
    "                    'degree': edu.get('area', ''),  # Using \"area\" as degree field\n",
    "                    'institution': edu.get('institution', ''),\n",
    "                    'graduation_year': edu.get('end', '')\n",
    "                }\n",
    "                \n",
    "                self.conn.execute(\n",
    "                    \"\"\"INSERT INTO education (education_id, candidate_id, degree, institution, graduation_year)\n",
    "                       VALUES ($education_id, $candidate_id, $degree, $institution, $graduation_year)\"\"\",\n",
    "                    edu_data\n",
    "                )\n",
    "            \n",
    "            # 4. Insert skills (normalized approach)\n",
    "            skills = structured_resume.get('skills', [])\n",
    "            for skill_name in skills:\n",
    "                if skill_name:  # Skip empty skills\n",
    "                    # Check if skill exists, if not create it\n",
    "                    existing_skill = self.conn.execute(\n",
    "                        \"SELECT skill_id FROM skills_master WHERE skill_name = ?\", [skill_name]\n",
    "                    ).fetchone()\n",
    "                    \n",
    "                    if existing_skill:\n",
    "                        skill_id = existing_skill[0]\n",
    "                    else:\n",
    "                        skill_id = self.conn.execute(\"SELECT nextval('skill_seq')\").fetchone()[0]\n",
    "                        self.conn.execute(\n",
    "                            \"INSERT INTO skills_master (skill_id, skill_name) VALUES (?, ?)\",\n",
    "                            [skill_id, skill_name]\n",
    "                        )\n",
    "                    \n",
    "                    # Link candidate to skill\n",
    "                    self.conn.execute(\n",
    "                        \"INSERT OR IGNORE INTO candidate_skills (candidate_id, skill_id) VALUES (?, ?)\",\n",
    "                        [candidate_id, skill_id]\n",
    "                    )\n",
    "            \n",
    "            # 5. Insert projects\n",
    "            projects = structured_resume.get('projects', [])\n",
    "            for project in projects:\n",
    "                project_id = self.conn.execute(\"SELECT nextval('project_seq')\").fetchone()[0]\n",
    "                \n",
    "                project_data = {\n",
    "                    'project_id': project_id,\n",
    "                    'candidate_id': candidate_id,\n",
    "                    'project_name': project.get('name', ''),\n",
    "                    'description': project.get('description', '')\n",
    "                }\n",
    "                \n",
    "                self.conn.execute(\n",
    "                    \"\"\"INSERT INTO projects (project_id, candidate_id, project_name, description)\n",
    "                       VALUES ($project_id, $candidate_id, $project_name, $description)\"\"\",\n",
    "                    project_data\n",
    "                )\n",
    "            \n",
    "            # Commit transaction\n",
    "            self.conn.commit()\n",
    "            logger.info(f\"Successfully imported resume data for candidate_id: {candidate_id}\")\n",
    "            return candidate_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Rollback on error\n",
    "            self.conn.rollback()\n",
    "            logger.error(f\"Error importing resume data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _parse_date(self, date_string: str) -> Optional[str]:\n",
    "        \"\"\"Parse date string to proper format.\"\"\"\n",
    "        if not date_string or date_string.lower() == 'present':\n",
    "            return None\n",
    "        \n",
    "        # Simple date parsing - can be enhanced with dateutil\n",
    "        try:\n",
    "            # Assume format YYYY-MM-DD\n",
    "            if len(date_string) == 10 and '-' in date_string:\n",
    "                return date_string\n",
    "            # Handle year-only format\n",
    "            elif len(date_string) == 4 and date_string.isdigit():\n",
    "                return f\"{date_string}-01-01\"\n",
    "            else:\n",
    "                return date_string\n",
    "        except:\n",
    "            return date_string\n",
    "    \n",
    "    def get_candidate_summary(self, candidate_id: int) -> Dict:\n",
    "        \"\"\"Get complete candidate information with joins.\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            c.candidate_id,\n",
    "            c.name,\n",
    "            c.email,\n",
    "            c.phone,\n",
    "            -- Work experience\n",
    "            we.job_title,\n",
    "            we.company,\n",
    "            we.start_date,\n",
    "            we.end_date,\n",
    "            -- Education\n",
    "            e.degree,\n",
    "            e.institution,\n",
    "            e.graduation_year,\n",
    "            -- Skills (aggregated)\n",
    "            GROUP_CONCAT(sm.skill_name) as skills,\n",
    "            -- Projects\n",
    "            p.project_name,\n",
    "            p.description as project_description\n",
    "        FROM candidates c\n",
    "        LEFT JOIN work_experience we ON c.candidate_id = we.candidate_id\n",
    "        LEFT JOIN education e ON c.candidate_id = e.candidate_id\n",
    "        LEFT JOIN candidate_skills cs ON c.candidate_id = cs.candidate_id\n",
    "        LEFT JOIN skills_master sm ON cs.skill_id = sm.skill_id\n",
    "        LEFT JOIN projects p ON c.candidate_id = p.candidate_id\n",
    "        WHERE c.candidate_id = ?\n",
    "        GROUP BY c.candidate_id, c.name, c.email, c.phone, we.job_title, we.company, \n",
    "                 we.start_date, we.end_date, e.degree, e.institution, e.graduation_year,\n",
    "                 p.project_name, p.description\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.conn.execute(query, [candidate_id]).fetchall()\n",
    "        return result\n",
    "    \n",
    "    def search_candidates_by_skill(self, skill_name: str) -> List:\n",
    "        \"\"\"Search candidates by skill.\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT DISTINCT c.candidate_id, c.name, c.email, c.phone\n",
    "        FROM candidates c\n",
    "        JOIN candidate_skills cs ON c.candidate_id = cs.candidate_id\n",
    "        JOIN skills_master sm ON cs.skill_id = sm.skill_id\n",
    "        WHERE LOWER(sm.skill_name) LIKE LOWER(?)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.conn.execute(query, [f\"%{skill_name}%\"]).fetchall()\n",
    "    \n",
    "    def get_database_stats(self) -> Dict:\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        \n",
    "        stats = {\n",
    "            'total_candidates': self.conn.execute(\"SELECT COUNT(*) FROM candidates\").fetchone()[0],\n",
    "            'total_work_experiences': self.conn.execute(\"SELECT COUNT(*) FROM work_experience\").fetchone()[0],\n",
    "            'total_educations': self.conn.execute(\"SELECT COUNT(*) FROM education\").fetchone()[0],\n",
    "            'total_skills': self.conn.execute(\"SELECT COUNT(*) FROM skills_master\").fetchone()[0],\n",
    "            'total_projects': self.conn.execute(\"SELECT COUNT(*) FROM projects\").fetchone()[0]\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0da2db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with CV Processing System\n",
    "def integrate_with_cv_processor():\n",
    "    \"\"\"Extended CV processor with database integration.\"\"\"\n",
    "    \n",
    "    # Initialize database\n",
    "    resume_db = ResumeDatabase(\"hr_resume_system.db\")\n",
    "    \n",
    "    # Sample usage - this would integrate with your CV processor\n",
    "    def process_and_store_resume(pdf_path: str) -> int:\n",
    "        \"\"\"Process resume PDF and store in database.\"\"\"\n",
    "        \n",
    "        # This would use your existing CVProcessor\n",
    "        cv_processor = CVProcessor()\n",
    "        structured_data = cv_processor.process_resume(pdf_path)\n",
    "        candidate_id = resume_db.import_resume_data(structured_data)\n",
    "        return candidate_id\n",
    "    return resume_db, process_and_store_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aec7359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Snowflake schema created successfully\n"
     ]
    }
   ],
   "source": [
    "resume_db, process_resume_func = integrate_with_cv_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abece144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Azure Document Intelligence extraction for: 10265057.pdf\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com//documentintelligence/documentModels/prebuilt-layout:analyze?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'content-type': 'application/octet-stream'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '18a45c10-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 202\n",
      "Response headers:\n",
      "    'Content-Length': '0'\n",
      "    'Operation-Location': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:55 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/09dc1e1e-0b82-4dda-af8a-42e3a870c85b?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '18a45c10-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '106'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'retry-after': '1'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:55 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://tritdocintel.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt-layout/analyzeResults/09dc1e1e-0b82-4dda-af8a-42e3a870c85b?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-client-request-id': '18a45c10-80f0-11f0-8a88-00155db623cc'\n",
      "    'User-Agent': 'azsdk-python-ai-documentintelligence/1.0.2 Python/3.11.13 (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)'\n",
      "    'Ocp-Apim-Subscription-Key': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '128157'\n",
      "    'Content-Type': 'application/json; charset=utf-8'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'Date': 'Sun, 24 Aug 2025 13:41:57 GMT'\n",
      "INFO:__main__:Found 20 paragraphs\n",
      "INFO:__main__:Successfully extracted 5154 characters using Azure Document Intelligence\n",
      "INFO:__main__:Starting LLM-based data structuring\n",
      "INFO:httpx:HTTP Request: POST https://yvstritopenai.openai.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2025-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "/tmp/ipykernel_46698/3678663438.py:27: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  structured_data = structured_resume.dict()\n",
      "INFO:__main__:Successfully structured resume data\n",
      "INFO:__main__:Successfully imported resume data for candidate_id: 1\n"
     ]
    }
   ],
   "source": [
    "candidate_id = process_resume_func(\"10265057.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8e1d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resume imported with candidate_id: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Resume imported with candidate_id: {candidate_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "014ac88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Candidate Summary: [(1, '', '', '', 'System Data Analyst', 'Company Name', None, None, '', 'PURDUE UNIVERSITY', '', 'big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation,big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation', 'The Design and Evaluation of a 5.8 GHz Laptop-Based Radar System', 'Innovative laptop radar design to operate in both FMCW and CW mode; Doppler shift (DTI), ranging (RTI), and SAR measurement capability; Operate in ISM frequency band with +13dBm transmitting power; Data acquisition and signal processing using Matlab.'), (1, '', '', '', 'Electrical/Validation Engineer', 'Company Name', None, None, '', 'PURDUE UNIVERSITY', '', 'big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation,big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation', 'The Design and Evaluation of a 5.8 GHz Laptop-Based Radar System', 'Innovative laptop radar design to operate in both FMCW and CW mode; Doppler shift (DTI), ranging (RTI), and SAR measurement capability; Operate in ISM frequency band with +13dBm transmitting power; Data acquisition and signal processing using Matlab.'), (1, '', '', '', 'Working RF Systems Engineer', 'Company Name', None, None, '', 'PURDUE UNIVERSITY', '', 'big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation,big data,C,C++,charts,Circuit design,hardware,Data acquisition,data analyst,data collection,data mining,databases,database,dBm,DTI,design software,documentation,functional,GSM,innovation,Java,LabView,Team leader,Logic Analyzer,Mac,manufacturing processes,Matlab,Excel,Microsoft office,Office,Microwave,Radar,NCs,Network,dB,packaging,pivot tables,Programming,project design,proposals,Publication,Python,quality,requirement,research,SAS,self-starter,Spectrum analyzer,SPSS,SQL,SSL,statistics,surveys,system design,troubleshooting,validation', 'The Design and Evaluation of a 5.8 GHz Laptop-Based Radar System', 'Innovative laptop radar design to operate in both FMCW and CW mode; Doppler shift (DTI), ranging (RTI), and SAR measurement capability; Operate in ISM frequency band with +13dBm transmitting power; Data acquisition and signal processing using Matlab.')]\n"
     ]
    }
   ],
   "source": [
    "summary = resume_db.get_candidate_summary(candidate_id)\n",
    "print(f\"📊 Candidate Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c6a9f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stats = \u001b[43mresume_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_database_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📈 Database Stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 296\u001b[39m, in \u001b[36mResumeDatabase.get_database_stats\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_database_stats\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Dict:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get database statistics.\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m     stats = {\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_candidates\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT COUNT(*) FROM candidates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.fetchone()[\u001b[32m0\u001b[39m],\n\u001b[32m    297\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_work_experiences\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.conn.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM work_experience\u001b[39m\u001b[33m\"\u001b[39m).fetchone()[\u001b[32m0\u001b[39m],\n\u001b[32m    298\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_educations\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.conn.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM education\u001b[39m\u001b[33m\"\u001b[39m).fetchone()[\u001b[32m0\u001b[39m],\n\u001b[32m    299\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_skills\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.conn.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM skills_master\u001b[39m\u001b[33m\"\u001b[39m).fetchone()[\u001b[32m0\u001b[39m],\n\u001b[32m    300\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_projects\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.conn.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM projects\u001b[39m\u001b[33m\"\u001b[39m).fetchone()[\u001b[32m0\u001b[39m]\n\u001b[32m    301\u001b[39m     }\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "\u001b[31mConnectionException\u001b[39m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "stats = resume_db.get_database_stats()\n",
    "print(f\"📈 Database Stats: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc354b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c23b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
